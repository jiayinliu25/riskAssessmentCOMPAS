{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.87      0.95      0.91       759\\n           1       0.70      0.45      0.55       190\\n\\n    accuracy                           0.85       949\\n   macro avg       0.79      0.70      0.73       949\\nweighted avg       0.84      0.85      0.84       949\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "compas = pd.read_csv('data/compas-scores-two-years-violent.csv')\n",
    "\n",
    "\n",
    "compas['reoffend'] = compas['v_decile_score'].apply(lambda x: 1 if x > 5 else 0)\n",
    "\n",
    "# select features and target\n",
    "features = compas[['age', 'priors_count']]\n",
    "target = compas['reoffend']\n",
    "\n",
    "# Splitting data\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, train_target)\n",
    "predicted_classes = model.predict(test_features)\n",
    "\n",
    "# Eval model\n",
    "accuracy = accuracy_score(test_target, predicted_classes)\n",
    "report = classification_report(test_target, predicted_classes)\n",
    "conf_matrix = confusion_matrix(test_target, predicted_classes)\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the old model: 0.8514225500526871\n",
      "Accuracy of the new model: 0.8061116965226555\n",
      "The old model performs better.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "compas['reoffend'] = compas['v_decile_score'].apply(lambda x: 1 if x > 5 else 0)\n",
    "features_old = compas[['age', 'priors_count']]\n",
    "target_old = compas['reoffend']\n",
    "train_features_old, test_features_old, train_target_old, test_target_old = train_test_split(features_old, target_old, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_old = LogisticRegression()\n",
    "model_old.fit(train_features_old, train_target_old)\n",
    "predicted_classes_old = model_old.predict(test_features_old)\n",
    "\n",
    "\n",
    "accuracy_old = accuracy_score(test_target_old, predicted_classes_old)\n",
    "print(\"Accuracy of the old model:\", accuracy_old)\n",
    "\n",
    "# Selecting features \n",
    "features_new = compas[['age', 'priors_count', 'race']]\n",
    "target_new = compas['reoffend']\n",
    "features_new = pd.get_dummies(features_new, columns=['race'], drop_first=True)\n",
    "train_features_new, test_features_new, train_target_new, test_target_new = train_test_split(features_new, target_new, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_new = LinearRegression()\n",
    "model_new.fit(train_features_new, train_target_new)\n",
    "predicted_values_new = model_new.predict(test_features_new)\n",
    "predicted_classes_new = [1 if val > 0.5 else 0 for val in predicted_values_new]\n",
    "\n",
    "\n",
    "accuracy_new = accuracy_score(test_target_new, predicted_classes_new)\n",
    "print(\"Accuracy of the new model:\", accuracy_new)\n",
    "if accuracy_new > accuracy_old:\n",
    "    print(\"The new model performs better.\")\n",
    "elif accuracy_new < accuracy_old:\n",
    "    print(\"The old model performs better.\")\n",
    "else:\n",
    "    print(\"Both models have the same accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+---------+\n",
      "|    Metric    |     Precision      |       Recall       |      F1-Score      | Support |\n",
      "+--------------+--------------------+--------------------+--------------------+---------+\n",
      "|   Class 0    | 0.8731884057971014 | 0.9525691699604744 | 0.9111531190926278 |  759.0  |\n",
      "|   Class 1    | 0.7024793388429752 | 0.4473684210526316 | 0.5466237942122186 |  190.0  |\n",
      "|   accuracy   |                    |                    | 0.8514225500526871 |  949.0  |\n",
      "|  macro avg   | 0.7878338723200383 | 0.699968795506553  | 0.7288884566524232 |  949.0  |\n",
      "| weighted avg | 0.8390106157852111 | 0.8514225500526871 | 0.8381704302335363 |  949.0  |\n",
      "+--------------+--------------------+--------------------+--------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# reformatting the classification report \n",
    "from tabulate import tabulate\n",
    "\n",
    "# Converting into a dictionary\n",
    "report_dict = classification_report(test_target, predicted_classes, output_dict=True)\n",
    "\n",
    "# data \n",
    "report_data = []\n",
    "for key, value in report_dict.items():\n",
    "    if key == 'accuracy':\n",
    "        report_data.append(['accuracy', '', '', value, report_dict['macro avg']['support']])\n",
    "    elif key in ['macro avg', 'weighted avg']:\n",
    "        report_data.append([key, value['precision'], value['recall'], value['f1-score'], value['support']])\n",
    "    else:\n",
    "        report_data.append([f'Class {key}', value['precision'], value['recall'], value['f1-score'], value['support']])\n",
    "\n",
    "\n",
    "headers = [\"Metric\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n",
    "table = tabulate(report_data, headers, tablefmt=\"pretty\")\n",
    "print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1: Likely to reoffend\n",
      "Individual 2: Less likely to reoffend\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [25, 40],\n",
    "    'priors_count': [10, 0]\n",
    "})\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(new_data)\n",
    "prediction_labels = ['Likely to reoffend' if pred == 1 else 'Less likely to reoffend' for pred in predictions]\n",
    "\n",
    "for i, label in enumerate(prediction_labels):\n",
    "    print(f\"Individual {i+1}: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
